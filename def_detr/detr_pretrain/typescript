Script started on 2022-12-08 23:18:34-05:00
(base) bash-4.4$ GPUS_PER_NODE=2 ./tools/run_dist_launch.sh 2 ./configs/DETReg_toop30_coco.sh --batch_size 8 --epochs 20 --num_workers 2[1P[1@1[C
+ GPUS=2
+ RUN_COMMAND='./configs/DETReg_top30_coco.sh --batch_size 8 --epochs 10 --num_workers 2'
+ '[' 2 -lt 8 ']'
+ GPUS_PER_NODE=2
+ MASTER_ADDR=127.0.0.1
+ MASTER_PORT=29500
+ NODE_RANK=0
+ let NNODES=GPUS/GPUS_PER_NODE
+ python ./tools/launch.py --nnodes 1 --node_rank 0 --master_addr 127.0.0.1 --master_port 29500 --nproc_per_node 2 ./configs/DETReg_top30_coco.sh --batch_size 8 --epochs 10 --num_workers 2
+ EXP_DIR=exps/DETReg_top30_coco
+ PY_ARGS='--batch_size 8 --epochs 10 --num_workers 2'
+ python -u main.py --output_dir exps/DETReg_top30_coco --dataset coco_pretrain --strategy topk --load_backbone swav --max_prop 30 --object_embedding_loss --lr_backbone 0 --batch_size 8 --epochs 10 --num_workers 2
+ EXP_DIR=exps/DETReg_top30_coco
+ PY_ARGS='--batch_size 8 --epochs 10 --num_workers 2'
+ python -u main.py --output_dir exps/DETReg_top30_coco --dataset coco_pretrain --strategy topk --load_backbone swav --max_prop 30 --object_embedding_loss --lr_backbone 0 --batch_size 8 --epochs 10 --num_workers 2
| distributed init (rank 1): env://
| distributed init (rank 0): env://
fatal: not a git repository (or any parent up to mount point /)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
fatal: not a git repository (or any parent up to mount point /)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
git:
  sha: N/A, status: clean, branch: N/A

Namespace(model='deformable_detr', lr=0.0002, max_prop=30, lr_backbone_names=['backbone.0'], lr_backbone=0.0, lr_linear_proj_names=['reference_points', 'sampling_offsets'], lr_linear_proj_mult=0.1, batch_size=8, weight_decay=0.0001, epochs=10, lr_drop=40, lr_drop_epochs=None, clip_max_norm=0.1, sgd=False, filter_pct=-1, filter_num=-1, reset_embedding_layer=1, with_box_refine=False, two_stage=False, strategy='topk', obj_embedding_head='intermediate', frozen_weights=None, backbone='resnet50', dilation=False, position_embedding='sine', position_embedding_scale=6.283185307179586, num_feature_levels=4, enc_layers=6, dec_layers=6, dim_feedforward=1024, hidden_dim=256, dropout=0.1, nheads=8, num_queries=300, dec_n_points=4, enc_n_points=4, pretrain='', load_backbone='swav', masks=False, aux_loss=True, set_cost_class=2, set_cost_bbox=5, set_cost_giou=2, object_embedding_coef=1, mask_loss_coef=1, dice_loss_coef=1, cls_loss_coef=2, bbox_loss_coef=5, giou_loss_coef=2, focal_alpha=0.25, dataset_file='coco', dataset='coco_pretrain', data_root='data', coco_panoptic_path=None, remove_difficult=False, output_dir='exps/DETReg_top30_coco', cache_path=None, device='cuda', seed=42, resume='', random_seed=False, eval_every=1, save_every=1, start_epoch=0, eval=False, viz=False, num_workers=2, cache_mode=False, object_embedding_loss=True, pre_norm=False, eos_coef=0.1, coco_path='/', rank=0, world_size=2, gpu=0, dist_url='env://', distributed=True, dist_backend='nccl')
Using random seed: 42
DATASETTTT ISSSS ----------------------------
coco_pretrain
/home/ds6812/miniconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/ds6812/miniconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/ds6812/miniconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
/home/ds6812/miniconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
number of params: 16814699
main.py 219 unlabeled ----------------------
num of files:5895
num of files:5895
transformer.level_embed
transformer.encoder.layers.0.self_attn.sampling_offsets.weight
transformer.encoder.layers.0.self_attn.sampling_offsets.bias
transformer.encoder.layers.0.self_attn.attention_weights.weight
transformer.encoder.layers.0.self_attn.attention_weights.bias
transformer.encoder.layers.0.self_attn.value_proj.weight
transformer.encoder.layers.0.self_attn.value_proj.bias
transformer.encoder.layers.0.self_attn.output_proj.weight
transformer.encoder.layers.0.self_attn.output_proj.bias
transformer.encoder.layers.0.norm1.weight
transformer.encoder.layers.0.norm1.bias
transformer.encoder.layers.0.linear1.weight
transformer.encoder.layers.0.linear1.bias
transformer.encoder.layers.0.linear2.weight
transformer.encoder.layers.0.linear2.bias
transformer.encoder.layers.0.norm2.weight
transformer.encoder.layers.0.norm2.bias
transformer.encoder.layers.1.self_attn.sampling_offsets.weight
transformer.encoder.layers.1.self_attn.sampling_offsets.bias
transformer.encoder.layers.1.self_attn.attention_weights.weight
transformer.encoder.layers.1.self_attn.attention_weights.bias
transformer.encoder.layers.1.self_attn.value_proj.weight
transformer.encoder.layers.1.self_attn.value_proj.bias
transformer.encoder.layers.1.self_attn.output_proj.weight
transformer.encoder.layers.1.self_attn.output_proj.bias
transformer.encoder.layers.1.norm1.weight
transformer.encoder.layers.1.norm1.bias
transformer.encoder.layers.1.linear1.weight
transformer.encoder.layers.1.linear1.bias
transformer.encoder.layers.1.linear2.weight
transformer.encoder.layers.1.linear2.bias
transformer.encoder.layers.1.norm2.weight
transformer.encoder.layers.1.norm2.bias
transformer.encoder.layers.2.self_attn.sampling_offsets.weight
transformer.encoder.layers.2.self_attn.sampling_offsets.bias
transformer.encoder.layers.2.self_attn.attention_weights.weight
transformer.encoder.layers.2.self_attn.attention_weights.bias
transformer.encoder.layers.2.self_attn.value_proj.weight
transformer.encoder.layers.2.self_attn.value_proj.bias
transformer.encoder.layers.2.self_attn.output_proj.weight
transformer.encoder.layers.2.self_attn.output_proj.bias
transformer.encoder.layers.2.norm1.weight
transformer.encoder.layers.2.norm1.bias
transformer.encoder.layers.2.linear1.weight
transformer.encoder.layers.2.linear1.bias
transformer.encoder.layers.2.linear2.weight
transformer.encoder.layers.2.linear2.bias
transformer.encoder.layers.2.norm2.weight
transformer.encoder.layers.2.norm2.bias
transformer.encoder.layers.3.self_attn.sampling_offsets.weight
transformer.encoder.layers.3.self_attn.sampling_offsets.bias
transformer.encoder.layers.3.self_attn.attention_weights.weight
transformer.encoder.layers.3.self_attn.attention_weights.bias
transformer.encoder.layers.3.self_attn.value_proj.weight
transformer.encoder.layers.3.self_attn.value_proj.bias
transformer.encoder.layers.3.self_attn.output_proj.weight
transformer.encoder.layers.3.self_attn.output_proj.bias
transformer.encoder.layers.3.norm1.weight
transformer.encoder.layers.3.norm1.bias
transformer.encoder.layers.3.linear1.weight
transformer.encoder.layers.3.linear1.bias
transformer.encoder.layers.3.linear2.weight
transformer.encoder.layers.3.linear2.bias
transformer.encoder.layers.3.norm2.weight
transformer.encoder.layers.3.norm2.bias
transformer.encoder.layers.4.self_attn.sampling_offsets.weight
transformer.encoder.layers.4.self_attn.sampling_offsets.bias
transformer.encoder.layers.4.self_attn.attention_weights.weight
transformer.encoder.layers.4.self_attn.attention_weights.bias
transformer.encoder.layers.4.self_attn.value_proj.weight
transformer.encoder.layers.4.self_attn.value_proj.bias
transformer.encoder.layers.4.self_attn.output_proj.weight
transformer.encoder.layers.4.self_attn.output_proj.bias
transformer.encoder.layers.4.norm1.weight
transformer.encoder.layers.4.norm1.bias
transformer.encoder.layers.4.linear1.weight
transformer.encoder.layers.4.linear1.bias
transformer.encoder.layers.4.linear2.weight
transformer.encoder.layers.4.linear2.bias
transformer.encoder.layers.4.norm2.weight
transformer.encoder.layers.4.norm2.bias
transformer.encoder.layers.5.self_attn.sampling_offsets.weight
transformer.encoder.layers.5.self_attn.sampling_offsets.bias
transformer.encoder.layers.5.self_attn.attention_weights.weight
transformer.encoder.layers.5.self_attn.attention_weights.bias
transformer.encoder.layers.5.self_attn.value_proj.weight
transformer.encoder.layers.5.self_attn.value_proj.bias
transformer.encoder.layers.5.self_attn.output_proj.weight
transformer.encoder.layers.5.self_attn.output_proj.bias
transformer.encoder.layers.5.norm1.weight
transformer.encoder.layers.5.norm1.bias
transformer.encoder.layers.5.linear1.weight
transformer.encoder.layers.5.linear1.bias
transformer.encoder.layers.5.linear2.weight
transformer.encoder.layers.5.linear2.bias
transformer.encoder.layers.5.norm2.weight
transformer.encoder.layers.5.norm2.bias
transformer.decoder.layers.0.cross_attn.sampling_offsets.weight
transformer.decoder.layers.0.cross_attn.sampling_offsets.bias
transformer.decoder.layers.0.cross_attn.attention_weights.weight
transformer.decoder.layers.0.cross_attn.attention_weights.bias
transformer.decoder.layers.0.cross_attn.value_proj.weight
transformer.decoder.layers.0.cross_attn.value_proj.bias
transformer.decoder.layers.0.cross_attn.output_proj.weight
transformer.decoder.layers.0.cross_attn.output_proj.bias
transformer.decoder.layers.0.norm1.weight
transformer.decoder.layers.0.norm1.bias
transformer.decoder.layers.0.self_attn.in_proj_weight
transformer.decoder.layers.0.self_attn.in_proj_bias
transformer.decoder.layers.0.self_attn.out_proj.weight
transformer.decoder.layers.0.self_attn.out_proj.bias
transformer.decoder.layers.0.norm2.weight
transformer.decoder.layers.0.norm2.bias
transformer.decoder.layers.0.linear1.weight
transformer.decoder.layers.0.linear1.bias
transformer.decoder.layers.0.linear2.weight
transformer.decoder.layers.0.linear2.bias
transformer.decoder.layers.0.norm3.weight
transformer.decoder.layers.0.norm3.bias
transformer.decoder.layers.1.cross_attn.sampling_offsets.weight
transformer.decoder.layers.1.cross_attn.sampling_offsets.bias
transformer.decoder.layers.1.cross_attn.attention_weights.weight
transformer.decoder.layers.1.cross_attn.attention_weights.bias
transformer.decoder.layers.1.cross_attn.value_proj.weight
transformer.decoder.layers.1.cross_attn.value_proj.bias
transformer.decoder.layers.1.cross_attn.output_proj.weight
transformer.decoder.layers.1.cross_attn.output_proj.bias
transformer.decoder.layers.1.norm1.weight
transformer.decoder.layers.1.norm1.bias
transformer.decoder.layers.1.self_attn.in_proj_weight
transformer.decoder.layers.1.self_attn.in_proj_bias
transformer.decoder.layers.1.self_attn.out_proj.weight
transformer.decoder.layers.1.self_attn.out_proj.bias
transformer.decoder.layers.1.norm2.weight
transformer.decoder.layers.1.norm2.bias
transformer.decoder.layers.1.linear1.weight
transformer.decoder.layers.1.linear1.bias
transformer.decoder.layers.1.linear2.weight
transformer.decoder.layers.1.linear2.bias
transformer.decoder.layers.1.norm3.weight
transformer.decoder.layers.1.norm3.bias
transformer.decoder.layers.2.cross_attn.sampling_offsets.weight
transformer.decoder.layers.2.cross_attn.sampling_offsets.bias
transformer.decoder.layers.2.cross_attn.attention_weights.weight
transformer.decoder.layers.2.cross_attn.attention_weights.bias
transformer.decoder.layers.2.cross_attn.value_proj.weight
transformer.decoder.layers.2.cross_attn.value_proj.bias
transformer.decoder.layers.2.cross_attn.output_proj.weight
transformer.decoder.layers.2.cross_attn.output_proj.bias
transformer.decoder.layers.2.norm1.weight
transformer.decoder.layers.2.norm1.bias
transformer.decoder.layers.2.self_attn.in_proj_weight
transformer.decoder.layers.2.self_attn.in_proj_bias
transformer.decoder.layers.2.self_attn.out_proj.weight
transformer.decoder.layers.2.self_attn.out_proj.bias
transformer.decoder.layers.2.norm2.weight
transformer.decoder.layers.2.norm2.bias
transformer.decoder.layers.2.linear1.weight
transformer.decoder.layers.2.linear1.bias
transformer.decoder.layers.2.linear2.weight
transformer.decoder.layers.2.linear2.bias
transformer.decoder.layers.2.norm3.weight
transformer.decoder.layers.2.norm3.bias
transformer.decoder.layers.3.cross_attn.sampling_offsets.weight
transformer.decoder.layers.3.cross_attn.sampling_offsets.bias
transformer.decoder.layers.3.cross_attn.attention_weights.weight
transformer.decoder.layers.3.cross_attn.attention_weights.bias
transformer.decoder.layers.3.cross_attn.value_proj.weight
transformer.decoder.layers.3.cross_attn.value_proj.bias
transformer.decoder.layers.3.cross_attn.output_proj.weight
transformer.decoder.layers.3.cross_attn.output_proj.bias
transformer.decoder.layers.3.norm1.weight
transformer.decoder.layers.3.norm1.bias
transformer.decoder.layers.3.self_attn.in_proj_weight
transformer.decoder.layers.3.self_attn.in_proj_bias
transformer.decoder.layers.3.self_attn.out_proj.weight
transformer.decoder.layers.3.self_attn.out_proj.bias
transformer.decoder.layers.3.norm2.weight
transformer.decoder.layers.3.norm2.bias
transformer.decoder.layers.3.linear1.weight
transformer.decoder.layers.3.linear1.bias
transformer.decoder.layers.3.linear2.weight
transformer.decoder.layers.3.linear2.bias
transformer.decoder.layers.3.norm3.weight
transformer.decoder.layers.3.norm3.bias
transformer.decoder.layers.4.cross_attn.sampling_offsets.weight
transformer.decoder.layers.4.cross_attn.sampling_offsets.bias
transformer.decoder.layers.4.cross_attn.attention_weights.weight
transformer.decoder.layers.4.cross_attn.attention_weights.bias
transformer.decoder.layers.4.cross_attn.value_proj.weight
transformer.decoder.layers.4.cross_attn.value_proj.bias
transformer.decoder.layers.4.cross_attn.output_proj.weight
transformer.decoder.layers.4.cross_attn.output_proj.bias
transformer.decoder.layers.4.norm1.weight
transformer.decoder.layers.4.norm1.bias
transformer.decoder.layers.4.self_attn.in_proj_weight
transformer.decoder.layers.4.self_attn.in_proj_bias
transformer.decoder.layers.4.self_attn.out_proj.weight
transformer.decoder.layers.4.self_attn.out_proj.bias
transformer.decoder.layers.4.norm2.weight
transformer.decoder.layers.4.norm2.bias
transformer.decoder.layers.4.linear1.weight
transformer.decoder.layers.4.linear1.bias
transformer.decoder.layers.4.linear2.weight
transformer.decoder.layers.4.linear2.bias
transformer.decoder.layers.4.norm3.weight
transformer.decoder.layers.4.norm3.bias
transformer.decoder.layers.5.cross_attn.sampling_offsets.weight
transformer.decoder.layers.5.cross_attn.sampling_offsets.bias
transformer.decoder.layers.5.cross_attn.attention_weights.weight
transformer.decoder.layers.5.cross_attn.attention_weights.bias
transformer.decoder.layers.5.cross_attn.value_proj.weight
transformer.decoder.layers.5.cross_attn.value_proj.bias
transformer.decoder.layers.5.cross_attn.output_proj.weight
transformer.decoder.layers.5.cross_attn.output_proj.bias
transformer.decoder.layers.5.norm1.weight
transformer.decoder.layers.5.norm1.bias
transformer.decoder.layers.5.self_attn.in_proj_weight
transformer.decoder.layers.5.self_attn.in_proj_bias
transformer.decoder.layers.5.self_attn.out_proj.weight
transformer.decoder.layers.5.self_attn.out_proj.bias
transformer.decoder.layers.5.norm2.weight
transformer.decoder.layers.5.norm2.bias
transformer.decoder.layers.5.linear1.weight
transformer.decoder.layers.5.linear1.bias
transformer.decoder.layers.5.linear2.weight
transformer.decoder.layers.5.linear2.bias
transformer.decoder.layers.5.norm3.weight
transformer.decoder.layers.5.norm3.bias
transformer.reference_points.weight
transformer.reference_points.bias
class_embed.0.weight
class_embed.0.bias
feature_embed.layers.0.weight
feature_embed.layers.0.bia